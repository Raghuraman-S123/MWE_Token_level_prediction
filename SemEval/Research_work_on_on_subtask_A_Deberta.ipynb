{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-TXGBemGgH"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WsITUAnzvFl"
      },
      "source": [
        "Download the Task data and evaluation scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3qhQdpl-1-",
        "outputId": "a561bffb-553a-4852-aee4-6c480b459fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval_2022_Task2-idiomaticity'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 123 (delta 48), reused 61 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 2.50 MiB | 9.89 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-0POB9tzfNx"
      },
      "source": [
        "Download the “AStitchInLanguageModels” code which we make use of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "affNQCRktdx4",
        "outputId": "c5e062f7-6c53-43af-91dc-ef71a85dd3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AStitchInLanguageModels'...\n",
            "remote: Enumerating objects: 1030, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 1030 (delta 11), reused 4 (delta 4), pack-reused 1013\u001b[K\n",
            "Receiving objects: 100% (1030/1030), 79.59 MiB | 24.01 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60w-An2vzikk"
      },
      "source": [
        "Download and install an editable version of huggingfaces transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8BhcLYcmVvd",
        "outputId": "9caeb7d5-3270-4da8-b384-2d297a02cb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 142421, done.\u001b[K\n",
            "remote: Counting objects: 100% (943/943), done.\u001b[K\n",
            "remote: Compressing objects: 100% (510/510), done.\u001b[K\n",
            "remote: Total 142421 (delta 488), reused 692 (delta 385), pack-reused 141478\u001b[K\n",
            "Receiving objects: 100% (142421/142421), 142.08 MiB | 24.48 MiB/s, done.\n",
            "Resolving deltas: 100% (106316/106316), done.\n",
            "/content/transformers\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.0.dev0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0.dev0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.2.1 (from transformers==4.30.0.dev0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (3.4)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.30.0.dev0-0.editable-py3-none-any.whl size=36621 sha256=9075bfc6ba3da4e43773cc40bd9624b81c90d9e3c3eca675feff9bf288310dc5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6fn57jkk/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.0.dev0\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVMnwTSzmjJ"
      },
      "source": [
        "Required for run_glue ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsWits5tw1t",
        "outputId": "5d24dc00-0bf2-44d2-ea0a-c4b89fb93ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "## run_glue needs this.\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxYxJctZ5pQz",
        "outputId": "4c71df3c-3edc-426c-8fb4-bbba1f6269bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m669.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.30.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=4ee463ee808c791aa1a5894efcaad2d4c0e72ef11946fbbe1eb667ec73c25a66\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-igYdTTgzp9e"
      },
      "source": [
        "Editable install requires runtime restart unless we do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOuKplBmmbeB"
      },
      "outputs": [],
      "source": [
        "import site\n",
        "site.main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvC8kAGNnKk_"
      },
      "source": [
        "# Imports and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOw3MaG7nN77"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "2-n6L1pigVey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzDtW9eXnOhG"
      },
      "outputs": [],
      "source": [
        "def load_csv( path, delimiter=',' ) :\n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter )\n",
        "    for row in reader :\n",
        "      if header is None :\n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row )\n",
        "  return header, data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwtDsdtAnSZu"
      },
      "outputs": [],
      "source": [
        "def write_csv( data, location ) :\n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile )\n",
        "    writer.writerows( data )\n",
        "  print( \"Wrote {}\".format( location ) )\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Io3D3_z4wt"
      },
      "source": [
        "The following function creates a submission file from the predictions output by run_glue (the text classification script from huggingface transformers - see below).\n",
        "\n",
        "Note that we set it up so we can load up results for only one setting.\n",
        "\n",
        "It requires as input the submission format file, which is available with the data. You can call this after completing each setting to load up results for both settings (see below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re31vnLoQWww"
      },
      "outputs": [],
      "source": [
        "def insert_to_submission_file( submission_format_file, input_file, prediction_format_file, setting ) :\n",
        "    submission_header, submission_content = load_csv( submission_format_file )\n",
        "    input_header     , input_data         = load_csv( input_file             )\n",
        "    prediction_header, prediction_data    = load_csv( prediction_format_file, '\\t' )\n",
        "\n",
        "    assert len( input_data ) == len( prediction_data )\n",
        "\n",
        "    ## submission_header ['ID', 'Language', 'Setting', 'Label']\n",
        "    ## input_header      ['label', 'sentence1' ]\n",
        "    ## prediction_header ['index', 'prediction']\n",
        "\n",
        "    prediction_data = list( reversed( prediction_data ) )\n",
        "\n",
        "    started_insert  = False\n",
        "    for elem in submission_content :\n",
        "        if elem[ submission_header.index( 'Setting' ) ] != setting :\n",
        "            if started_insert :\n",
        "                if len( prediction_data ) == 0 :\n",
        "                    break\n",
        "                else :\n",
        "                    raise Exception( \"Update should to contiguous ... something wrong.\" )\n",
        "            continue\n",
        "        started_insert = True\n",
        "        elem[ submission_header.index( 'Label' ) ] = prediction_data.pop()[ prediction_header.index( 'prediction' ) ]\n",
        "\n",
        "    return [ submission_header ] + submission_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44LyZ-OXmgQW"
      },
      "source": [
        "# Pre-process: Create train and dev and evaluation data in required format\n",
        "\n",
        "In the zero-shot setting, we choose to include the context (the sentences preceding and succeeding the one containing the idioms). We do not add the idiom as an additional feature (in the “second input sentence”).\n",
        "\n",
        "In the one shot setting, we train the model on both the zero-shot and one-shot data. In this setting, we exclude the context (the sentences preceding and succeeding the one containing the idioms) and also add the idiom as an additional feature in the “second sentence”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3ymBcEmxaV"
      },
      "source": [
        "## Functions for pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MthVK7EQm6m_"
      },
      "source": [
        "### _get_train_data\n",
        "\n",
        "This function generates training data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPGq-Y1Jmvv5"
      },
      "outputs": [],
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom,language ) :\n",
        "\n",
        "    file_name = os.path.join( data_location, file_name )\n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if language==\"EN\":\n",
        "          if elem[header.index('Language')] != language:\n",
        "            continue\n",
        "        if language==\"PT\":\n",
        "          if elem[header.index('Language')] != language:\n",
        "            continue\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ]\n",
        "        else :\n",
        "            sentence2 = elem[ header.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    fin=[out_header] + out_data\n",
        "    return fin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytociCB3WZM"
      },
      "source": [
        "### _get_dev_eval_data\n",
        "\n",
        "This function generates training dev and eval data in the format required by the huggingface’s example script. It will include and exclude the MWE and the context based on parameters.\n",
        "\n",
        "Additionally, if there is no gold label provides (as in the case of eval) it will generate a file that can be used to generate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe4YQJ9Sm-B2"
      },
      "outputs": [],
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None :\n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label', 'sentence1', 'sentence2' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None :\n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "\n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "\n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ]\n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        assert len( out_header ) == len( this_row )\n",
        "        out_data.append( this_row )\n",
        "\n",
        "\n",
        "    return [ out_header ] + out_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjIbyTnn3fHP"
      },
      "source": [
        "### create_data\n",
        "\n",
        "This function generates the training, development and evaluation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1tr-zNvnBCV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Based on the results presented in `AStitchInLanguageModels' we work with not including the idiom for the zero shot setting and including it in the one shot setting.\n",
        "\"\"\"\n",
        "def create_data( input_location, output_location ) :\n",
        "\n",
        "\n",
        "    ## Zero shot data\n",
        "    train_data_multi = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False,\n",
        "        language        = 'Multi'\n",
        "    )\n",
        "    write_csv( train_data_multi, os.path.join( output_location, 'ZeroShot', 'train_multi.csv' ) )\n",
        "\n",
        "    train_data_en = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False,\n",
        "        language        = 'EN'\n",
        "    )\n",
        "    write_csv( train_data_en, os.path.join( output_location, 'ZeroShot', 'train_en.csv' ) )\n",
        "\n",
        "    train_data_PT = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False,\n",
        "        language        = 'PT'\n",
        "    )\n",
        "    write_csv( train_data_PT, os.path.join( output_location, 'ZeroShot', 'train_pt.csv' ) )\n",
        "\n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv',\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "\n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n"
      ],
      "metadata": {
        "id": "T5OuCPK2gg42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_multi=pd.read_csv(\"/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/train_zero_shot.csv\")\n",
        "#train_multi_df=pd.DataFrame(train_multi)\n",
        "X=train_multi.drop('Language',axis=1)\n",
        "y=train_multi['Language']\n",
        "under=RandomUnderSampler(sampling_strategy='majority')\n",
        "X_over, y_over = under.fit_resample(X, y)\n",
        "train_multi_undersampled = pd.concat([X_over, y_over], axis=1, join='inner')\n",
        "train_en_undersampled=train_multi_undersampled[train_multi_undersampled['Language']=='EN']\n",
        "filepath=''\n",
        "train_en_undersampled.to_csv('Data/ZeroShot/train_en_undersampled.csv')\n",
        "train_data_multi = _get_train_data(\n",
        "        data_location   = 'Data/ZeroShot/',\n",
        "        file_name       = 'train_en_undersampled.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False,\n",
        "        language        = 'Multi'\n",
        "    )\n",
        "write_csv( train_data_multi, os.path.join( 'Data', 'ZeroShot', 'train_en_undersampled_final.csv' ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYgoE55zfvKh",
        "outputId": "f30f4a38-0f36-48ec-e400-59cb2cf27e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/train_en_undersampled_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmQfvym8ndKH"
      },
      "source": [
        "## Setup and Create data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCgaHlKnpMR",
        "outputId": "7ea00684-c4ce-41dd-959a-f64e891e6345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AStitchInLanguageModels  SemEval_2022_Task2-idiomaticity\n",
            "sample_data\t\t transformers\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkeKLg-Hngs4",
        "outputId": "92369807-d236-461a-ffe2-06204b10422b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/train.csv\n",
            "Wrote Data/ZeroShot/train_pt.csv\n",
            "Wrote Data/ZeroShot/train_en.csv\n",
            "Wrote Data/ZeroShot/dev.csv\n"
          ]
        }
      ],
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mchr1wFJzOC1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Based on the results presented in `AStitchInLanguageModels' we work with not including the idiom for the zero shot setting and including it in the one shot setting.\n",
        "\"\"\"\n",
        "def create_data( input_location, output_location ) :\n",
        "\n",
        "\n",
        "    test_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'test.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( test_data, os.path.join( output_location, 'ZeroShot', 'test.csv' ) )\n",
        "\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kij4KuEMzY73",
        "outputId": "cbc26e38-9478-4c93-e78b-97d0fd67cdbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Data/ZeroShot/test.csv\n"
          ]
        }
      ],
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/TestData/', outpath )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-Ol7hfoC8a"
      },
      "source": [
        "# Zero Shot Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-GiQvnkoL67"
      },
      "source": [
        "## Train Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqIgJ1s8ojiI",
        "outputId": "6225468c-f581-4384-abe2-3a286f54cade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0BwA0uoKAu",
        "outputId": "b7e9d6ea-9d30-424d-b646-674db17984e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-26 19:56:44.772716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/26/2023 19:56:47 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "05/26/2023 19:56:47 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/5/runs/May26_19-56-47_493202a2e116,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/ZeroShot/5/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/5/,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/26/2023 19:56:47 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train_en_undersampled_final.csv\n",
            "05/26/2023 19:56:47 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-91197a52b166f57f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 10880.17it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1444.32it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-91197a52b166f57f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 940.95it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 579/579 [00:00<00:00, 2.97MB/s]\n",
            "[INFO|configuration_utils.py:669] 2023-05-26 19:56:47,927 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 19:56:47,937 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/mdeberta-v3-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.30.0.dev0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 251000\n",
            "}\n",
            "\n",
            "Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 322kB/s]\n",
            "[INFO|configuration_utils.py:669] 2023-05-26 19:56:48,137 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 19:56:48,138 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/mdeberta-v3-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.30.0.dev0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 251000\n",
            "}\n",
            "\n",
            "Downloading spm.model: 100% 4.31M/4.31M [00:00<00:00, 77.8MB/s]\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-26 19:56:48,682 >> loading file spm.model from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/spm.model\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-26 19:56:48,682 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-26 19:56:48,682 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-26 19:56:48,682 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1810] 2023-05-26 19:56:48,682 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:669] 2023-05-26 19:56:48,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 19:56:48,683 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/mdeberta-v3-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.30.0.dev0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 251000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:426] 2023-05-26 19:56:49,788 >> Adding [MASK] to the vocabulary\n",
            "[WARNING|logging.py:280] 2023-05-26 19:56:49,788 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:669] 2023-05-26 19:56:49,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 19:56:49,789 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"microsoft/mdeberta-v3-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"transformers_version\": \"4.30.0.dev0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 251000\n",
            "}\n",
            "\n",
            "/content/transformers/src/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "[WARNING|logging.py:280] 2023-05-26 19:56:50,481 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Downloading pytorch_model.bin: 100% 1.33G/1.33G [00:04<00:00, 275MB/s]\n",
            "[INFO|modeling_utils.py:2535] 2023-05-26 19:56:55,819 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--mdeberta-v3-base/snapshots/a0484667b22365f84929a935b5e50a51f71f159d/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:3229] 2023-05-26 19:56:58,881 >> Some weights of the model checkpoint at microsoft/mdeberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
            "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3241] 2023-05-26 19:56:58,881 >> Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "05/26/2023 19:56:59 - INFO - __main__ -   Sample 788 of the training set: {'label': 0, 'sentence1': 'Trump to announce further US troop withdrawals from Iraq, Afghanistan Afghanistan frees nearly 200 Taliban prisoners to push peace talks Iran voices concern about deadly attacks in Afghanistan, offers help in negotiations', 'input_ids': [1, 4397, 289, 260, 103110, 260, 17757, 2280, 260, 118959, 260, 75098, 5156, 703, 260, 36987, 262, 73262, 73262, 1653, 264, 9138, 485, 1951, 367, 153896, 260, 54921, 1208, 289, 35006, 48084, 260, 138243, 19256, 260, 56000, 14217, 1389, 270, 143077, 260, 107218, 282, 73262, 262, 260, 5761, 2115, 282, 260, 159130, 264, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "05/26/2023 19:56:59 - INFO - __main__ -   Sample 861 of the training set: {'label': 0, 'sentence1': 'The Edwardian novelist and biographer E. F. Benson recalled such an evening at a country house party: After the hostess and the other ladies had bid goodnight to their husbands, most of the men changed their evening coats for smoking jackets, though one or two did not bother to do this, but went as they were into the smoking room, where presently the others joined them, wearing their braided and frogged habiliments. Lord Buryan made a more complete change and appeared in a suit of ruby-coloured velvet.', 'input_ids': [1, 487, 34499, 4326, 20234, 1906, 306, 838, 167630, 287, 416, 261, 516, 261, 3841, 1683, 585, 13076, 3393, 462, 260, 46532, 345, 260, 263, 11396, 6957, 13900, 268, 11077, 288, 624, 13508, 306, 288, 1905, 260, 66397, 1426, 32071, 2317, 27995, 289, 260, 1617, 260, 43907, 264, 262, 2251, 305, 288, 693, 260, 36486, 260, 1617, 260, 46532, 81312, 264, 333, 260, 100502, 260, 202195, 262, 16534, 1372, 632, 2957, 3032, 777, 6118, 296, 289, 343, 715, 262, 1157, 260, 14197, 528, 288, 277, 2110, 2388, 288, 260, 100502, 6369, 262, 260, 3002, 4964, 485, 288, 14036, 16854, 346, 2487, 262, 25973, 348, 260, 1617, 4220, 526, 346, 306, 38355, 31646, 383, 28715, 10842, 261, 23472, 9421, 2483, 3786, 260, 263, 1098, 8594, 6314, 306, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "05/26/2023 19:56:59 - INFO - __main__ -   Sample 82 of the training set: {'label': 1, 'sentence1': 'BBC Production chief executive Matthew Bannister praised Windsor\\'s portrayal of Peggy coming to terms with a mastectomy, commenting: \"It\\'s brought a good deal of comfort and help to us and a lot of other people.\" Oncology nurses and consultants were involved in the development of the storyline, which was based on a real life case study. In Clive Seale\\'s book, Health and the Media, EastEnders was praised for putting its message across without being \"gruelling\".', 'input_ids': [1, 19673, 35447, 260, 48592, 260, 60181, 25782, 155592, 31337, 2503, 18807, 260, 127474, 278, 264, 520, 74961, 474, 305, 364, 200529, 260, 9703, 289, 28274, 515, 260, 263, 127531, 173588, 262, 4211, 348, 268, 314, 11781, 278, 264, 331, 35715, 260, 263, 2317, 19870, 305, 30905, 306, 2115, 289, 260, 439, 306, 260, 263, 4762, 305, 1905, 2560, 2455, 1642, 113925, 260, 85362, 264, 306, 260, 136343, 2110, 260, 38862, 282, 288, 10031, 305, 288, 9892, 1398, 262, 260, 1543, 640, 260, 5622, 352, 260, 263, 2785, 4257, 4073, 10381, 261, 564, 372, 10913, 62161, 266, 278, 264, 3436, 262, 9048, 306, 288, 4921, 262, 9196, 31967, 1208, 640, 2503, 18807, 333, 422, 37252, 2477, 13614, 260, 15260, 260, 5768, 260, 5331, 314, 319, 738, 110937, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "[INFO|trainer.py:763] 2023-05-26 19:57:05,161 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/content/transformers/src/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1812] 2023-05-26 19:57:05,171 >> ***** Running training *****\n",
            "[INFO|trainer.py:1813] 2023-05-26 19:57:05,171 >>   Num examples = 1,164\n",
            "[INFO|trainer.py:1814] 2023-05-26 19:57:05,171 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1815] 2023-05-26 19:57:05,171 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1816] 2023-05-26 19:57:05,171 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1817] 2023-05-26 19:57:05,171 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1818] 2023-05-26 19:57:05,171 >>   Total optimization steps = 333\n",
            "[INFO|trainer.py:1819] 2023-05-26 19:57:05,171 >>   Number of trainable parameters = 278,810,882\n",
            " 11% 37/333 [00:13<01:12,  4.10it/s][INFO|trainer.py:763] 2023-05-26 19:57:18,882 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:57:18,884 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:57:18,884 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:57:18,884 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 38.56it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:02, 30.83it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:02, 32.32it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:02, 33.43it/s]\u001b[A\n",
            " 23% 21/93 [00:00<00:02, 34.29it/s]\u001b[A\n",
            " 27% 25/93 [00:00<00:01, 34.73it/s]\u001b[A\n",
            " 31% 29/93 [00:00<00:01, 35.00it/s]\u001b[A\n",
            " 35% 33/93 [00:00<00:01, 34.32it/s]\u001b[A\n",
            " 40% 37/93 [00:01<00:01, 32.99it/s]\u001b[A\n",
            " 44% 41/93 [00:01<00:01, 33.61it/s]\u001b[A\n",
            " 48% 45/93 [00:01<00:01, 33.86it/s]\u001b[A\n",
            " 53% 49/93 [00:01<00:01, 34.07it/s]\u001b[A\n",
            " 57% 53/93 [00:01<00:01, 34.15it/s]\u001b[A\n",
            " 61% 57/93 [00:01<00:01, 34.38it/s]\u001b[A\n",
            " 66% 61/93 [00:01<00:00, 34.45it/s]\u001b[A\n",
            " 70% 65/93 [00:01<00:00, 34.37it/s]\u001b[A\n",
            " 74% 69/93 [00:02<00:00, 34.48it/s]\u001b[A\n",
            " 78% 73/93 [00:02<00:00, 34.72it/s]\u001b[A\n",
            " 83% 77/93 [00:02<00:00, 34.92it/s]\u001b[A\n",
            " 87% 81/93 [00:02<00:00, 34.90it/s]\u001b[A\n",
            " 91% 85/93 [00:02<00:00, 35.12it/s]\u001b[A\n",
            " 96% 89/93 [00:02<00:00, 34.84it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6903477311134338, 'eval_accuracy': 0.510148823261261, 'eval_f1': 0.43141280691015443, 'eval_runtime': 2.7381, 'eval_samples_per_second': 269.891, 'eval_steps_per_second': 33.965, 'epoch': 1.0}\n",
            " 11% 37/333 [00:16<01:12,  4.10it/s]\n",
            "100% 93/93 [00:02<00:00, 35.41it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:57:21,623 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-37\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:57:21,624 >> Configuration saved in models/ZeroShot/5/checkpoint-37/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:57:25,111 >> Model weights saved in models/ZeroShot/5/checkpoint-37/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:57:25,112 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-37/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:57:25,112 >> Special tokens file saved in models/ZeroShot/5/checkpoint-37/special_tokens_map.json\n",
            " 22% 74/333 [00:44<01:02,  4.11it/s][INFO|trainer.py:763] 2023-05-26 19:57:50,130 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:57:50,132 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:57:50,132 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:57:50,132 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.36it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 38.10it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.67it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 36.18it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:01, 35.92it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 35.64it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 35.52it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 35.22it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 35.05it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 35.18it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 34.79it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 34.90it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 34.92it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:00, 35.17it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 35.14it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 35.03it/s]\u001b[A\n",
            " 75% 70/93 [00:01<00:00, 35.00it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 35.03it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 35.15it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 35.22it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 35.07it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.6344974637031555, 'eval_accuracy': 0.672530472278595, 'eval_f1': 0.6674884724081511, 'eval_runtime': 2.6568, 'eval_samples_per_second': 278.151, 'eval_steps_per_second': 35.004, 'epoch': 2.0}\n",
            " 22% 74/333 [00:47<01:02,  4.11it/s]\n",
            "100% 93/93 [00:02<00:00, 35.10it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:57:52,790 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-74\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:57:52,791 >> Configuration saved in models/ZeroShot/5/checkpoint-74/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:57:56,445 >> Model weights saved in models/ZeroShot/5/checkpoint-74/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:57:56,445 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-74/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:57:56,446 >> Special tokens file saved in models/ZeroShot/5/checkpoint-74/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 19:58:08,682 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-37] due to args.save_total_limit\n",
            " 33% 111/333 [01:14<00:53,  4.11it/s][INFO|trainer.py:763] 2023-05-26 19:58:19,665 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:58:19,667 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:58:19,667 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:58:19,667 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.97it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 37.81it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.49it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 35.79it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:02, 35.43it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 35.24it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 35.13it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 35.16it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 35.07it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 35.04it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 34.98it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 35.01it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 35.07it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:00, 35.06it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 35.14it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 35.16it/s]\u001b[A\n",
            " 75% 70/93 [00:01<00:00, 35.18it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 35.09it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 34.92it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 34.99it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 34.74it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.6693451404571533, 'eval_accuracy': 0.6806495189666748, 'eval_f1': 0.6651486060978419, 'eval_runtime': 2.6651, 'eval_samples_per_second': 277.287, 'eval_steps_per_second': 34.895, 'epoch': 3.0}\n",
            " 33% 111/333 [01:17<00:53,  4.11it/s]\n",
            "100% 93/93 [00:02<00:00, 34.56it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:58:22,333 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-111\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:58:22,334 >> Configuration saved in models/ZeroShot/5/checkpoint-111/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:58:26,052 >> Model weights saved in models/ZeroShot/5/checkpoint-111/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:58:26,052 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-111/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:58:26,053 >> Special tokens file saved in models/ZeroShot/5/checkpoint-111/special_tokens_map.json\n",
            " 44% 148/333 [01:44<00:45,  4.11it/s][INFO|trainer.py:763] 2023-05-26 19:58:49,425 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:58:49,427 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:58:49,427 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:58:49,427 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.41it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 38.06it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.58it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 35.98it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:01, 35.60it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 35.39it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 35.26it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 35.39it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 35.36it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 35.33it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 35.14it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 35.31it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 35.37it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:00, 35.39it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 35.36it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 35.36it/s]\u001b[A\n",
            " 75% 70/93 [00:01<00:00, 35.36it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 35.35it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 35.31it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 35.17it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 34.92it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8121423721313477, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6456554890031285, 'eval_runtime': 2.6488, 'eval_samples_per_second': 278.991, 'eval_steps_per_second': 35.11, 'epoch': 4.0}\n",
            " 44% 148/333 [01:46<00:45,  4.11it/s]\n",
            "100% 93/93 [00:02<00:00, 34.94it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:58:52,077 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-148\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:58:52,078 >> Configuration saved in models/ZeroShot/5/checkpoint-148/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:58:55,700 >> Model weights saved in models/ZeroShot/5/checkpoint-148/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:58:55,701 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-148/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:58:55,701 >> Special tokens file saved in models/ZeroShot/5/checkpoint-148/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 19:59:04,755 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-111] due to args.save_total_limit\n",
            " 56% 185/333 [02:10<00:36,  4.09it/s][INFO|trainer.py:763] 2023-05-26 19:59:15,663 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:59:15,665 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:59:15,665 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:59:15,665 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.58it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 37.07it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 35.88it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 34.83it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:02, 34.52it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 34.26it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 33.91it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 33.70it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 33.69it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 33.69it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 33.92it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 33.88it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 33.60it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:01, 33.77it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 33.62it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 33.40it/s]\u001b[A\n",
            " 75% 70/93 [00:02<00:00, 33.66it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 33.68it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 33.63it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 33.51it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 33.42it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.0984106063842773, 'eval_accuracy': 0.6549391150474548, 'eval_f1': 0.622706295736438, 'eval_runtime': 2.7586, 'eval_samples_per_second': 267.892, 'eval_steps_per_second': 33.713, 'epoch': 5.0}\n",
            " 56% 185/333 [02:13<00:36,  4.09it/s]\n",
            "100% 93/93 [00:02<00:00, 33.52it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:59:18,428 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-185\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:59:18,429 >> Configuration saved in models/ZeroShot/5/checkpoint-185/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:59:22,054 >> Model weights saved in models/ZeroShot/5/checkpoint-185/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:59:22,054 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-185/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:59:22,055 >> Special tokens file saved in models/ZeroShot/5/checkpoint-185/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 19:59:34,977 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-148] due to args.save_total_limit\n",
            " 67% 222/333 [02:40<00:26,  4.11it/s][INFO|trainer.py:763] 2023-05-26 19:59:45,886 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 19:59:45,889 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 19:59:45,889 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 19:59:45,889 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 42.77it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 37.82it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.77it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 35.85it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:02, 35.41it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 34.68it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 34.67it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 34.64it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 34.90it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 34.72it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 34.84it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 34.62it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 34.57it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:01, 34.58it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 34.29it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 34.49it/s]\u001b[A\n",
            " 75% 70/93 [00:01<00:00, 34.37it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 34.14it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 33.97it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 34.32it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 34.64it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1285403966903687, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6451143091069405, 'eval_runtime': 2.7012, 'eval_samples_per_second': 273.583, 'eval_steps_per_second': 34.429, 'epoch': 6.0}\n",
            " 67% 222/333 [02:43<00:26,  4.11it/s]\n",
            "100% 93/93 [00:02<00:00, 34.13it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 19:59:48,591 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-222\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 19:59:48,592 >> Configuration saved in models/ZeroShot/5/checkpoint-222/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 19:59:52,177 >> Model weights saved in models/ZeroShot/5/checkpoint-222/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 19:59:52,178 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 19:59:52,178 >> Special tokens file saved in models/ZeroShot/5/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 20:00:01,183 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-185] due to args.save_total_limit\n",
            " 78% 259/333 [03:06<00:17,  4.12it/s][INFO|trainer.py:763] 2023-05-26 20:00:12,117 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:00:12,119 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:00:12,119 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:00:12,119 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.93it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 38.37it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.87it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 36.21it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:01, 35.84it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 35.22it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 35.21it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 35.11it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 35.22it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 34.89it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 34.38it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 34.32it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 34.55it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:01, 34.37it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 34.40it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 34.53it/s]\u001b[A\n",
            " 75% 70/93 [00:01<00:00, 33.88it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 33.93it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 33.93it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 33.95it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 34.19it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.3015201091766357, 'eval_accuracy': 0.6738836169242859, 'eval_f1': 0.6464079732372415, 'eval_runtime': 2.6948, 'eval_samples_per_second': 274.235, 'eval_steps_per_second': 34.511, 'epoch': 7.0}\n",
            " 78% 259/333 [03:09<00:17,  4.12it/s]\n",
            "100% 93/93 [00:02<00:00, 34.36it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 20:00:14,815 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-259\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 20:00:14,816 >> Configuration saved in models/ZeroShot/5/checkpoint-259/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 20:00:18,820 >> Model weights saved in models/ZeroShot/5/checkpoint-259/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 20:00:18,821 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-259/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 20:00:18,821 >> Special tokens file saved in models/ZeroShot/5/checkpoint-259/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 20:00:34,429 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-222] due to args.save_total_limit\n",
            " 89% 296/333 [03:40<00:09,  4.10it/s][INFO|trainer.py:763] 2023-05-26 20:00:45,327 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:00:45,329 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:00:45,329 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:00:45,329 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.41it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 37.14it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 35.85it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 34.94it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:02, 34.52it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 34.42it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 34.49it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 34.06it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 34.34it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 34.23it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 34.41it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 34.40it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 34.43it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:01, 34.06it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:00, 34.07it/s]\u001b[A\n",
            " 71% 66/93 [00:01<00:00, 33.64it/s]\u001b[A\n",
            " 75% 70/93 [00:02<00:00, 33.90it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 33.84it/s]\u001b[A\n",
            " 84% 78/93 [00:02<00:00, 33.83it/s]\u001b[A\n",
            " 88% 82/93 [00:02<00:00, 33.26it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 33.77it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.379604697227478, 'eval_accuracy': 0.672530472278595, 'eval_f1': 0.6540047980188826, 'eval_runtime': 2.7351, 'eval_samples_per_second': 270.189, 'eval_steps_per_second': 34.002, 'epoch': 8.0}\n",
            " 89% 296/333 [03:42<00:09,  4.10it/s]\n",
            "100% 93/93 [00:02<00:00, 33.91it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 20:00:48,066 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-296\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 20:00:48,067 >> Configuration saved in models/ZeroShot/5/checkpoint-296/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 20:00:51,712 >> Model weights saved in models/ZeroShot/5/checkpoint-296/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 20:00:51,713 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-296/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 20:00:51,713 >> Special tokens file saved in models/ZeroShot/5/checkpoint-296/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 20:01:00,657 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-259] due to args.save_total_limit\n",
            "100% 333/333 [04:06<00:00,  4.10it/s][INFO|trainer.py:763] 2023-05-26 20:01:11,639 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:01:11,641 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:01:11,641 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:01:11,641 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 5/93 [00:00<00:02, 43.80it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:02, 38.39it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:02, 36.20it/s]\u001b[A\n",
            " 19% 18/93 [00:00<00:02, 35.34it/s]\u001b[A\n",
            " 24% 22/93 [00:00<00:02, 35.15it/s]\u001b[A\n",
            " 28% 26/93 [00:00<00:01, 34.98it/s]\u001b[A\n",
            " 32% 30/93 [00:00<00:01, 34.74it/s]\u001b[A\n",
            " 37% 34/93 [00:00<00:01, 34.19it/s]\u001b[A\n",
            " 41% 38/93 [00:01<00:01, 32.90it/s]\u001b[A\n",
            " 45% 42/93 [00:01<00:01, 32.64it/s]\u001b[A\n",
            " 49% 46/93 [00:01<00:01, 32.06it/s]\u001b[A\n",
            " 54% 50/93 [00:01<00:01, 31.25it/s]\u001b[A\n",
            " 58% 54/93 [00:01<00:01, 30.67it/s]\u001b[A\n",
            " 62% 58/93 [00:01<00:01, 30.34it/s]\u001b[A\n",
            " 67% 62/93 [00:01<00:01, 30.37it/s]\u001b[A\n",
            " 71% 66/93 [00:02<00:00, 30.25it/s]\u001b[A\n",
            " 75% 70/93 [00:02<00:00, 30.18it/s]\u001b[A\n",
            " 80% 74/93 [00:02<00:00, 29.75it/s]\u001b[A\n",
            " 83% 77/93 [00:02<00:00, 29.36it/s]\u001b[A\n",
            " 86% 80/93 [00:02<00:00, 29.18it/s]\u001b[A\n",
            " 89% 83/93 [00:02<00:00, 29.26it/s]\u001b[A\n",
            " 92% 86/93 [00:02<00:00, 28.67it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.4318976402282715, 'eval_accuracy': 0.6698240637779236, 'eval_f1': 0.6445658687356104, 'eval_runtime': 2.9703, 'eval_samples_per_second': 248.798, 'eval_steps_per_second': 31.31, 'epoch': 9.0}\n",
            "100% 333/333 [04:09<00:00,  4.10it/s]\n",
            "100% 93/93 [00:02<00:00, 29.38it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2956] 2023-05-26 20:01:14,613 >> Saving model checkpoint to models/ZeroShot/5/checkpoint-333\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 20:01:14,614 >> Configuration saved in models/ZeroShot/5/checkpoint-333/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 20:01:18,906 >> Model weights saved in models/ZeroShot/5/checkpoint-333/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 20:01:18,907 >> tokenizer config file saved in models/ZeroShot/5/checkpoint-333/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 20:01:18,908 >> Special tokens file saved in models/ZeroShot/5/checkpoint-333/special_tokens_map.json\n",
            "[INFO|trainer.py:3041] 2023-05-26 20:01:27,487 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-296] due to args.save_total_limit\n",
            "[INFO|trainer.py:2085] 2023-05-26 20:01:27,913 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2218] 2023-05-26 20:01:27,913 >> Loading best model from models/ZeroShot/5/checkpoint-74 (score: 0.6674884724081511).\n",
            "{'train_runtime': 268.4495, 'train_samples_per_second': 39.024, 'train_steps_per_second': 1.24, 'train_loss': 0.2848500890416784, 'epoch': 9.0}\n",
            "100% 333/333 [04:28<00:00,  4.10it/s][INFO|trainer.py:2119] 2023-05-26 20:01:33,630 >> Deleting older checkpoint [models/ZeroShot/5/checkpoint-333] due to args.save_total_limit\n",
            "100% 333/333 [04:28<00:00,  1.24it/s]\n",
            "[INFO|trainer.py:2956] 2023-05-26 20:01:34,050 >> Saving model checkpoint to models/ZeroShot/5/\n",
            "[INFO|configuration_utils.py:458] 2023-05-26 20:01:34,052 >> Configuration saved in models/ZeroShot/5/config.json\n",
            "[INFO|modeling_utils.py:1842] 2023-05-26 20:01:37,863 >> Model weights saved in models/ZeroShot/5/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2181] 2023-05-26 20:01:37,864 >> tokenizer config file saved in models/ZeroShot/5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2188] 2023-05-26 20:01:37,864 >> Special tokens file saved in models/ZeroShot/5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.2849\n",
            "  train_runtime            = 0:04:28.44\n",
            "  train_samples            =       1164\n",
            "  train_samples_per_second =     39.024\n",
            "  train_steps_per_second   =       1.24\n",
            "05/26/2023 20:01:38 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:763] 2023-05-26 20:01:38,330 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:01:38,332 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:01:38,332 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:01:38,332 >>   Batch size = 8\n",
            "100% 93/93 [00:02<00:00, 35.31it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.6725\n",
            "  eval_f1                 =     0.6675\n",
            "  eval_loss               =     0.6345\n",
            "  eval_runtime            = 0:00:02.66\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    277.527\n",
            "  eval_steps_per_second   =     34.926\n"
          ]
        }
      ],
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'microsoft/mdeberta-v3-base' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/5/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train_en_undersampled_final.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrfwDiORPDyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c949cee-dbc9-48b8-80c7-75821327f8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibb2Uo0vPPc3"
      },
      "outputs": [],
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/5/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/ZeroShot/5/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etL-Ic6bPmtA"
      },
      "outputs": [],
      "source": [
        "## Bring back saved model here.\n",
        "#!mkdir -p /content/models/ZeroShot/0/\n",
        "# !cp -r /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/* /content/models/ZeroShot/0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bN4iUHWP45b"
      },
      "source": [
        "## Evaluation On Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHqPYuTS3muJ"
      },
      "source": [
        "### Use predictions to create the submission file (for dev data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXcRbv70RZfR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ets4flitTRZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513031ef-690f-4651-e91b-cab1fea11c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-26 20:06:22.131905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/26/2023 20:06:24 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "05/26/2023 20:06:24 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/5/eval-eval/runs/May26_20-06-24_493202a2e116,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=models/ZeroShot/5/eval-eval/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/5/eval-eval/,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/26/2023 20:06:24 - INFO - __main__ -   load a local file for train: Data/ZeroShot/train_en_undersampled_final.csv\n",
            "05/26/2023 20:06:24 - INFO - __main__ -   load a local file for validation: Data/ZeroShot/dev.csv\n",
            "05/26/2023 20:06:24 - INFO - __main__ -   load a local file for test: Data/ZeroShot/test.csv\n",
            "05/26/2023 20:06:24 - WARNING - datasets.builder -   Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-963839764ae3df6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 3/3 [00:00<00:00, 830.50it/s]\n",
            "[INFO|configuration_utils.py:667] 2023-05-26 20:06:24,937 >> loading configuration file /content/models/ZeroShot/5/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-05-26 20:06:24,942 >> Model config DebertaV2Config {\n",
            "  \"_name_or_path\": \"/content/models/ZeroShot/5\",\n",
            "  \"architectures\": [\n",
            "    \"DebertaV2ForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta-v2\",\n",
            "  \"norm_rel_ebd\": \"layer_norm\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"p2c\",\n",
            "    \"c2p\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"position_buckets\": 256,\n",
            "  \"relative_attention\": true,\n",
            "  \"share_att_key\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.30.0.dev0\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 251000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 20:06:24,943 >> loading file spm.model\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 20:06:24,943 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 20:06:24,943 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 20:06:24,943 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1808] 2023-05-26 20:06:24,943 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2532] 2023-05-26 20:06:25,533 >> loading weights file /content/models/ZeroShot/5/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3239] 2023-05-26 20:06:46,085 >> All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:3247] 2023-05-26 20:06:46,085 >> All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at /content/models/ZeroShot/5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
            "05/26/2023 20:06:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-963839764ae3df6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-2ba9ed232805fd9e.arrow\n",
            "05/26/2023 20:06:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-963839764ae3df6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-73569365615e3b49.arrow\n",
            "05/26/2023 20:06:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-963839764ae3df6b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-6915c85b9fc85940.arrow\n",
            "05/26/2023 20:06:48 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:763] 2023-05-26 20:06:48,049 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:06:48,050 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:06:48,050 >>   Num examples = 739\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:06:48,051 >>   Batch size = 8\n",
            "100% 93/93 [00:02<00:00, 35.19it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6725\n",
            "  eval_f1                 =     0.6675\n",
            "  eval_loss               =     0.6345\n",
            "  eval_runtime            = 0:00:02.99\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    246.843\n",
            "  eval_steps_per_second   =     31.064\n",
            "05/26/2023 20:06:51 - INFO - __main__ -   *** Test ***\n",
            "[INFO|trainer.py:763] 2023-05-26 20:06:51,047 >> The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1. If sentence1 are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3217] 2023-05-26 20:06:51,049 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3219] 2023-05-26 20:06:51,049 >>   Num examples = 2342\n",
            "[INFO|trainer.py:3222] 2023-05-26 20:06:51,049 >>   Batch size = 8\n",
            "100% 293/293 [00:08<00:00, 34.72it/s]\n",
            "05/26/2023 20:06:59 - INFO - __main__ -   ***** Test results None *****\n"
          ]
        }
      ],
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/ZeroShot/5' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/5/eval-eval/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train_en_undersampled_final.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "      --test_file Data/ZeroShot/test.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSS6PFfb4AAl"
      },
      "source": [
        "### Use predictions to create the submission file (for eval data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqxzrRBfTcnq"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'submission_format_file' : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/TestData/test_submission_format.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/TestData/test.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/ZeroShot/5/eval-eval/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'zero_shot'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHqanuVDTz-r"
      },
      "outputs": [],
      "source": [
        " updated_data = insert_to_submission_file( **params )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKDBuxMLT5QU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee2ba3b-7505-4504-b1de-86dd9e34ffcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/zero_shot_test_mdeberta_undersampled.csv\n"
          ]
        }
      ],
      "source": [
        "write_csv( updated_data, 'outputs/zero_shot_test_mdeberta_undersampled.csv' )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOIM_tIVgCHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4qMOXU01PUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "70435f63-1d7e-429a-9677-63fe0b2bd65d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dc2d3686-228f-4496-9bc3-0a09a593e671\", \"zero_shot_test_mdeberta_undersampled.csv\", 97917)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/outputs/zero_shot_test_mdeberta_undersampled.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}